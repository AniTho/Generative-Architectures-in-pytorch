{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ Simple autoencoder motivated to use variational autoencoder which instead of learning latent dimension, it learns latent distribution of the data at hand by assuming data follows gaussian distribution and thus learns only mean and log of variance. \n",
    "\n",
    "Ref for why gaussian is used: https://stats.stackexchange.com/questions/402569/why-do-we-use-gaussian-distributions-in-variational-autoencoder\n",
    "\n",
    "Log of variance is taken since the variance can only be positive thus restrictive to learn on the other hand log of variance can range from $\\left(-\\infty, \\infty\\right)$ which does not restrict network to learn values. To learn mean and log of variance, KL Divergence was used since it measures the distance between two distribution along with reconstruction loss. And using the learned mean and log of variance sampling can be done and thus generate required data, this is done via (here z denotes latent dimension):\n",
    "\n",
    "$$\n",
    "z_{new} = \\mu_{z} + \\sigma_{z} * \\epsilon \\\\\n",
    "\\text{where} \\,\\, \\epsilon \\sim \\mathcal{N}\\left(0, 1\\right) \\\\\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "It is known that $\\log{\\left(\\sigma_{z}^2\\right)}$ say $z_{lvar}$ is learned so to get $\\sigma_{z}$. We know:\n",
    "\n",
    "$$\n",
    "\\sigma_{z} = \\exp^{\\left(\\log{\\left(\\sigma_{z}\\right)}\\right)} \\\\\n",
    "\n",
    "\\text{Multiplying and dividing by 2} \\\\\n",
    "\n",
    "\\sigma_{z} = \\exp^{\\left(\\frac{2}{2}\\log{\\left(\\sigma_{z}\\right)}\\right)} \\\\\n",
    "\n",
    "\\sigma_{z} = \\exp^{\\left(\\frac{2\\log{\\left(\\sigma_{z}\\right)}}{2}\\right)} \\\\\n",
    "\n",
    "\\sigma_{z} = \\exp^{\\left(\\frac{\\log{\\left(\\sigma_{z}^2\\right)}}{2}\\right)} \\\\\n",
    "\n",
    "\\sigma_{z} = \\exp^{\\left(0.5z_{lvar}\\right)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import wandb\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "def load_checkpoint(model, save_path):\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "def visualization(dataloader, model, title, clear = True):\n",
    "    model.eval()\n",
    "    encoder = model.encoder\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    num_examples = {i:10 for i in range(10)}\n",
    "    fig_1, axes_1 = plt.subplots(nrows = 10, ncols = 10, figsize = (10,10))\n",
    "    fig_2, axes_2 = plt.subplots(nrows = 10, ncols = 10, figsize = (10,10))\n",
    "    embed = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, total = len(dataloader), leave = False)\n",
    "        for imgs, lbls in pbar:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            imgs_inp = imgs.view(-1, 1, 28, 28)\n",
    "            embeddings = encoder(imgs_inp)\n",
    "            out = sigmoid(model(imgs))\n",
    "            out = out.view(len(out), 1, 28, 28)\n",
    "            embeddings = embeddings.view(len(out), -1)\n",
    "            embed.extend(embeddings.cpu().detach().numpy().tolist())\n",
    "            labels.extend(lbls.cpu().detach().numpy().tolist())\n",
    "            for n in num_examples.keys():\n",
    "                if num_examples[n] <= 0:\n",
    "                    continue\n",
    "                idxs = torch.where(lbls == n)[0][:num_examples[n]]\n",
    "                num_examples[n] -= len(idxs)\n",
    "                for idx in idxs:\n",
    "                    idx = idx.item()\n",
    "                    img = imgs[idx].detach().cpu().numpy()[0]\n",
    "                    gen = out[idx].detach().cpu().numpy()[0]\n",
    "                    axes_1[i//10, i%10].imshow(img, cmap = 'gray')\n",
    "                    axes_1[i//10, i%10].axis('off')\n",
    "                    axes_2[i//10, i%10].imshow(gen, cmap = 'gray')\n",
    "                    axes_2[i//10, i%10].axis('off')\n",
    "                    i+=1\n",
    "\n",
    "    fig_3, axes = plt.subplots()\n",
    "    embed = np.array(embed)\n",
    "    labels = np.array(labels)\n",
    "    color = plt.get_cmap('Spectral', 10)\n",
    "    scatter_plot = axes.scatter(embed[:,0], embed[:,1], c = labels, cmap = color)\n",
    "    plt.colorbar(scatter_plot, drawedges = True, ax = axes)\n",
    "    # wandb.log({f\"{title}_original\": fig_1, f\"{title}_regenerated\":fig_2, f\"{title}_visualization\": wandb.Image(fig_3)})\n",
    "    if clear:\n",
    "        fig_1.clear()\n",
    "        plt.close(fig_1)\n",
    "        fig_2.clear()\n",
    "        plt.close(fig_2)\n",
    "        fig_3.clear()\n",
    "        plt.close(fig_3)\n",
    "\n",
    "def KL_divergence(latent_mean, latent_log_var):\n",
    "    example_divergence = torch.sum(-0.5*(1 + latent_log_var - torch.square(latent_mean) - torch.exp(latent_log_var)), dim = -1)\n",
    "    batch_divergence = torch.mean(example_divergence)\n",
    "    return batch_divergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL Divergence Derivation (univariate gaussian):\n",
    "$$\n",
    "\n",
    "Given: \\\\\n",
    "P \\sim \\mathcal{N}\\left(\\mu, \\sigma \\right) \\\\\n",
    "Q \\sim \\mathcal{N}\\left(0, 1 \\right) \\\\\n",
    "\n",
    "D_{KL} \\left(p ||q\\right) = \\int^{\\infty}_{-\\infty} \\left(p(x)\\log{\\frac{p(x)}{q(x)}}\\right) dx \\\\\n",
    "\n",
    "= \\int^{\\infty}_{-\\infty} \\left(p(x)\\log{\\frac{\\frac{1}{\\cancel{\\sqrt{2\\pi}}\\sigma}\\exp^{\\left(-\\frac{\\left(x - \\mu\\right)^2}{2\\sigma^2}\\right)}}{\\frac{1}{\\cancel{\\sqrt{2\\pi}}}\\exp^{\\left(-\\frac{\\left(x\\right)^2}{2}\\right)}}}\\right) dx \\\\\n",
    "\n",
    "= \\int^{\\infty}_{-\\infty} \\left(p(x)\\log{\\frac{\\frac{1}{\\sigma}\\exp^{\\left(-\\frac{\\left(x - \\mu\\right)^2}{2\\sigma^2}\\right)}}{\\exp^{\\left(-\\frac{\\left(x\\right)^2}{2}\\right)}}}\\right) dx \\\\\n",
    "\n",
    "= \\int^{\\infty}_{-\\infty} \\left(p(x)\\log\\left(\\frac{1}{\\sigma}\\exp^{\\left(-\\frac{\\left(x - \\mu\\right)^2}{2\\sigma^2}\\right)}\\right)\\right) dx- \\int^{\\infty}_{-\\infty} \\left(p(x)\\log\\left(\\exp^{\\left(-\\frac{\\left(x\\right)^2}{2}\\right)}\\right)\\right) dx \\\\\n",
    "\n",
    "= \\int^{\\infty}_{-\\infty} \\left(p(x)\\log\\left(\\frac{1}{\\sigma}\\exp^{\\left(-\\frac{\\left(x - \\mu\\right)^2}{2\\sigma^2}\\right)}\\right)\\right)dx- \\int^{\\infty}_{-\\infty} \\left(p(x)\\left(-\\frac{\\left(x\\right)^2}{2}\\right)\\right) dx \\\\\n",
    "\n",
    "= \\int^{\\infty}_{-\\infty} \\left(p(x) \\log\\left(\\frac{1}{\\sigma}\\right)\\right)dx + \\int^{\\infty}_{-\\infty} \\left(p(x) \\log\\left(\\exp^{\\left(-\\frac{\\left(x^2 + \\mu^2 - 2x\\mu\\right)}{2\\sigma^2}\\right)}\\right) \\right)dx + \\frac{1}{2}\\int^{\\infty}_{-\\infty} \\left(x^2p(x)\\right) dx \\\\\n",
    "\n",
    "[\\text{Sidenote:} \\,\\,\\,\\, \\mathbb{E}\\left[x\\right] = \\int^{\\infty}_{-\\infty} (x.p(x))dx = \\mu\\\\\n",
    "\\mathbb{E}\\left[x^2\\right] = \\int^{\\infty}_{-\\infty} (x^2p(x))dx \\\\\n",
    "\\text{Now we know that: } \\mathbb{E}\\left[(x-\\mu)^2\\right] = \\sigma^2 (variance) = \\mathbb{E}\\left[x^2\\right] - \\left(\\mathbb{E}\\left[x\\right]\\right)^2 \\\\ \n",
    "= \\mathbb{E}\\left[x^2\\right] = \\sigma^2 + \\mu^2\\,\\,\\,\\,] \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right)\\int^{\\infty}_{-\\infty} \\left(p(x)\\right)dx + \\int^{\\infty}_{-\\infty} \\left(p(x) \\left(-\\frac{\\left(x^2 + \\mu^2 - 2x\\mu\\right)}{2\\sigma^2}\\right) \\right)dx + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "\\text{Since p(x) is a pdf, summing over entire x is 1} \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2\\sigma^2}\\int^{\\infty}_{-\\infty}p(x) \\left(x^2 + \\mu^2 - 2x\\mu\\right)dx + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2\\sigma^2}\\left(\\int^{\\infty}_{-\\infty}p(x)x^2dx + \\mu^2\\int^{\\infty}_{-\\infty}p(x)dx - 2\\mu\\int^{\\infty}_{-\\infty}p(x)xdx\\right) + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2\\sigma^2}\\left(\\sigma^2 + \\mu^2 + \\mu^2 - 2\\mu^2\\right) + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2\\sigma^2}\\left(\\sigma^2 + \\cancel{\\mu^2} + \\cancel{\\mu^2} + \\cancel{2\\mu^2}\\right) - \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2\\cancel{\\sigma^2}}\\left(\\cancel{\\sigma^2}\\right) + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\log\\left(\\frac{1}{\\sigma}\\right) - \\frac{1}{2} + \\frac{1}{2} \\left(\\sigma^2 + \\mu^2\\right) \\\\\n",
    "\n",
    "= \\frac{1}{2}\\left(-2\\log(\\sigma) - 1 + \\sigma^2 + \\mu^2 \\right) \\\\\n",
    "\n",
    "= -\\frac{1}{2}\\left(2\\log(\\sigma) + 1 - \\sigma^2 - \\mu^2 \\right) \\\\\n",
    "\n",
    "= -\\frac{1}{2}\\left(\\log(\\sigma^2) + 1 - \\sigma^2 - \\mu^2 \\right)\n",
    "$$\n",
    "\n",
    "And the above derived is the final formula for calculating KL divergence loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
